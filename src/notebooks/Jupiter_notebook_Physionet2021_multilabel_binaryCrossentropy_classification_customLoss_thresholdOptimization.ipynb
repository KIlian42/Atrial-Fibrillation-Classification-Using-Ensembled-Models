{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UPiaiebnCYov"
      },
      "outputs": [],
      "source": [
        "# == Hyperparameter configuration ==\n",
        "\n",
        "# Official scored labels Physionet 2021: https://github.com/physionetchallenges/evaluation-2021/blob/main/dx_mapping_scored.csv\n",
        "\n",
        "# 0 = 426783006 -> sinus rhythm (SR)\n",
        "# 1 = 164889003 -> atrial fibrillation (AF)\n",
        "# 2 = 164890007 -> atrial flutter (AFL)\n",
        "# 3 = 284470004 or 63593006 -> premature atrial contraction (PAC) or supraventricular premature beats (SVPB)\n",
        "# 4 = 427172004 or 17338001 -> premature ventricular contractions (PVC), ventricular premature beats (VPB)\n",
        "# 5 = 6374002 -> bundle branch block (BBB)\n",
        "# 6 = 426627000 -> bradycardia (Brady)\n",
        "# 7 = 733534002 or 164909002 -> complete left bundle branch block (CLBBB), left bundle branch block (LBBB)\n",
        "# 8 = 713427006 or 59118001 -> complete right bundle branch block (CRBBB), right bundle branch block (RBBB)\n",
        "# 9 = 270492004 -> 1st degree av block (IAVB)\n",
        "# 10 = 713426002 -> incomplete right bundle branch block (IRBBB)\n",
        "# 11 = 39732003 -> left axis deviation (LAD)\n",
        "# 12 = 445118002 -> left anterior fascicular block (LAnFB)\n",
        "# 13 = 251146004 -> low qrs voltages (LQRSV)\n",
        "# 14 = 698252002 -> nonspecific intraventricular conduction disorder (NSIVCB)\n",
        "# 15 = 10370003 -> pacing rhythm (PR)\n",
        "# 16 = 365413008 -> poor R wave Progression (PRWP)\n",
        "# 17 = 164947007 -> prolonged pr interval (LPR)\n",
        "# 18 = 111975006 -> prolonged qt interval (LQT)\n",
        "# 19 = 164917005 -> qwave abnormal (QAb)\n",
        "# 20 = 47665007 -> right axis deviation (RAD)\n",
        "# 21 = 427393009 -> sinus arrhythmia (SA)\n",
        "# 22 = 426177001 -> sinus bradycardia (SB)\n",
        "# 23 = 427084000 -> sinus tachycardia (STach)\n",
        "# 24 = 164934002 -> t wave abnormal (TAb)\n",
        "# 25 = 59931005 -> t wave inversion (TInv)\n",
        "\n",
        "VALID_LABELS = set(\n",
        "    [\n",
        "        \"164889003\",\n",
        "        \"164890007\",\n",
        "        \"6374002\",\n",
        "        \"426627000\",\n",
        "        \"733534002\",\n",
        "        \"713427006\",\n",
        "        \"270492004\",\n",
        "        \"713426002\",\n",
        "        \"39732003\",\n",
        "        \"445118002\",\n",
        "        \"164909002\",\n",
        "        \"251146004\",\n",
        "        \"698252002\",\n",
        "        \"426783006\",\n",
        "        \"284470004\",\n",
        "        \"10370003\",\n",
        "        \"365413008\",\n",
        "        \"427172004\",\n",
        "        \"164947007\",\n",
        "        \"111975006\",\n",
        "        \"164917005\",\n",
        "        \"47665007\",\n",
        "        \"59118001\",\n",
        "        \"427393009\",\n",
        "        \"426177001\",\n",
        "        \"427084000\",\n",
        "        \"63593006\",\n",
        "        \"164934002\",\n",
        "        \"59931005\",\n",
        "        \"17338001\",\n",
        "    ]\n",
        ")\n",
        "# VALID_LABELS = set([\"426783006\", \"164889003\", \"164890007\", \"284470004\", \"427172004\"]) # SR, AF, AFL, PAC, PVC\n",
        "NUM_CLASSES =  26\n",
        "\n",
        "CLASS_BALANCE = 6000 # imblearn undersampling & imblearn oversampling\n",
        "TEST_BALANCE = 100 # imblearn undersampling\n",
        "TRAIN_TEST_SPLIT = 0.8\n",
        "VALIDATION_SPLIT = 0.25\n",
        "\n",
        "EPOCHS = 500\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "68ZDikRKCZ6_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8ba413a-0063-4117-cae5-c3c1701b8def"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jul 18 20:22:41 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# == Check if GPU is available ==\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fEnUK8J5zqBe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d1fd87e-8afa-4736-ae1d-6b45a9fc4db8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-colab in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: google-auth==2.27.0 in /usr/local/lib/python3.10/dist-packages (from google-colab) (2.27.0)\n",
            "Requirement already satisfied: ipykernel==5.5.6 in /usr/local/lib/python3.10/dist-packages (from google-colab) (5.5.6)\n",
            "Requirement already satisfied: ipyparallel==8.8.0 in /usr/local/lib/python3.10/dist-packages (from google-colab) (8.8.0)\n",
            "Requirement already satisfied: ipython==7.34.0 in /usr/local/lib/python3.10/dist-packages (from google-colab) (7.34.0)\n",
            "Requirement already satisfied: notebook==6.5.5 in /usr/local/lib/python3.10/dist-packages (from google-colab) (6.5.5)\n",
            "Requirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.10/dist-packages (from google-colab) (2.0.3)\n",
            "Requirement already satisfied: portpicker==1.5.2 in /usr/local/lib/python3.10/dist-packages (from google-colab) (1.5.2)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from google-colab) (2.31.0)\n",
            "Requirement already satisfied: tornado==6.3.3 in /usr/local/lib/python3.10/dist-packages (from google-colab) (6.3.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0->google-colab) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0->google-colab) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0->google-colab) (4.9)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6->google-colab) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6->google-colab) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6->google-colab) (6.1.12)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipyparallel==8.8.0->google-colab) (4.4.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from ipyparallel==8.8.0->google-colab) (0.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipyparallel==8.8.0->google-colab) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from ipyparallel==8.8.0->google-colab) (2.8.2)\n",
            "Requirement already satisfied: pyzmq>=18 in /usr/local/lib/python3.10/dist-packages (from ipyparallel==8.8.0->google-colab) (24.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from ipyparallel==8.8.0->google-colab) (4.66.4)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython==7.34.0->google-colab)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (4.9.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (3.1.4)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (0.20.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->google-colab) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->google-colab) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->google-colab) (1.25.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->google-colab) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->google-colab) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->google-colab) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->google-colab) (2024.7.4)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython==7.34.0->google-colab) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook==6.5.5->google-colab) (4.2.2)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook==6.5.5->google-colab) (0.2.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (24.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook==6.5.5->google-colab) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook==6.5.5->google-colab) (4.19.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython==7.34.0->google-colab) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.34.0->google-colab) (0.2.13)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth==2.27.0->google-colab) (0.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->ipyparallel==8.8.0->google-colab) (1.16.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook==6.5.5->google-colab) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (0.19.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook==6.5.5->google-colab) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook==6.5.5->google-colab) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook==6.5.5->google-colab) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook==6.5.5->google-colab) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.2.2)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.19.1\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Collecting imblearn\n",
            "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (from imblearn) (0.10.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
            "Installing collected packages: imblearn\n",
            "Successfully installed imblearn-0.0\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# == Install requirements ==\n",
        "\n",
        "!pip install google-colab\n",
        "!pip install tensorflow keras numpy\n",
        "!pip install h5py tqdm\n",
        "!pip install pandas scipy imblearn\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UD4mNESiD965"
      },
      "outputs": [],
      "source": [
        "# == Import requirements ==\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import h5py\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "import random\n",
        "import scipy\n",
        "from scipy.signal import butter, lfilter\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rcryqS0FDqBV"
      },
      "outputs": [],
      "source": [
        "# == Preprocess functions ==\n",
        "\n",
        "def pad_or_truncate_ecg(ecg: list, max_samples: int) -> list:\n",
        "    try:\n",
        "        padded_or_truncated_ecg = ecg[:max_samples] + [0] * (max_samples - len(ecg))\n",
        "    except Exception as e:\n",
        "        print(\"Fail: padding\", e)\n",
        "    return padded_or_truncated_ecg\n",
        "\n",
        "def resample_ecg(ecg: list, resample: int):\n",
        "    new_ecg = scipy.signal.resample(\n",
        "        ecg, resample, t=None, axis=0, window=None, domain=\"time\"\n",
        "    )\n",
        "    return list(new_ecg)\n",
        "\n",
        "def normalize_to_minus11(ecg: list):\n",
        "    max_val = max(ecg)\n",
        "    min_val = min(ecg)\n",
        "    # Handle the case where max_val and min_val are the same (to avoid division by zero)\n",
        "    if max_val == min_val:\n",
        "        return [0 for _ in ecg]\n",
        "    normalized_values = [2 * (x - min_val) / (max_val - min_val) - 1 for x in ecg]\n",
        "    return normalized_values\n",
        "\n",
        "def butter_bandpass(lowcut, highcut, fs, order=4):\n",
        "    nyquist = 0.5 * fs\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "    b, a = butter(order, [low, high], btype=\"band\")\n",
        "    return b, a\n",
        "\n",
        "def butter_bandpass_filter(ecg: list, lowcut: float, highcut: float, sampling_rate: int, order: int =4):\n",
        "    b, a = butter_bandpass(lowcut, highcut, sampling_rate, order=order)\n",
        "    y = lfilter(b, a, ecg)\n",
        "    return y\n",
        "\n",
        "def split_list_into_n_sublists(lst, n):\n",
        "    k, m = divmod(len(lst), n)\n",
        "    return [lst[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# == Map labels to numerical values functions ==\n",
        "\n",
        "# Official scored labels Physionet 2021: https://github.com/physionetchallenges/evaluation-2021/blob/main/dx_mapping_scored.csv\n",
        "\n",
        "arrhyhtmia_mapping_id_to_index = {\n",
        "    \"426783006\": 0, # sinus rhythm (SR)\n",
        "    \"164889003\": 1, # atrial fibrillation (AF)\n",
        "    \"164890007\": 2, # atrial flutter (AFL)\n",
        "    \"284470004\": 3, # premature atrial contraction (PAC)\n",
        "    \"63593006\": 3, # supraventricular premature beats (SVPB)\n",
        "    \"427172004\": 4, # premature ventricular contractions (PVC)\n",
        "    \"17338001\": 4, # ventricular premature beats (VPB)\n",
        "    \"6374002\": 5, # bundle branch block (BBB)\n",
        "    \"426627000\": 6, # bradycardia (Brady)\n",
        "    \"733534002\": 7, # complete left bundle branch block (CLBBB)\n",
        "    \"164909002\": 7, # left bundle branch block (LBBB)\n",
        "    \"713427006\": 8, # complete right bundle branch block (CRBBB)\n",
        "    \"59118001\": 8, # right bundle branch block (RBBB)\n",
        "    \"270492004\": 9, # 1st degree av block (IAVB)\n",
        "    \"713426002\": 10, # incomplete right bundle branch block (IRBBB)\n",
        "    \"39732003\": 11, # left axis deviation (LAD)\n",
        "    \"445118002\": 12, # left anterior fascicular block (LAnFB)\n",
        "    \"251146004\": 13, # low qrs voltages (LQRSV)\n",
        "    \"698252002\": 14, # nonspecific intraventricular conduction disorder (NSIVCB)\n",
        "    \"10370003\": 15, # pacing rhythm (PR)\n",
        "    \"365413008\": 16, # poor R wave Progression (PRWP)\n",
        "    \"164947007\": 17, # prolonged pr interval (LPR)\n",
        "    \"111975006\": 18, # prolonged qt interval (LQT)\n",
        "    \"164917005\": 19, # qwave abnormal (QAb)\n",
        "    \"47665007\": 20,  # right axis deviation (RAD)\n",
        "    \"427393009\": 21, # sinus arrhythmia (SA)\n",
        "    \"426177001\": 22, # sinus bradycardia (SB)\n",
        "    \"427084000\": 23, # sinus tachycardia (STach)\n",
        "    \"164934002\": 24, # t wave abnormal (TAb)\n",
        "    \"59931005\": 25 # t wave inversion (TInv)\n",
        "}\n",
        "\n",
        "def map_arrhyhtmia_id_to_index(x: str) -> int:\n",
        "    return arrhyhtmia_mapping_id_to_index[x]\n",
        "\n",
        "arrhyhtmia_mapping_index_to_id = {\n",
        "    0: \"426783006\", # sinus rhythm (SR)\n",
        "    1: \"164889003\", # atrial fibrillation (AF)\n",
        "    2: \"164890007\", # atrial flutter (AFL)\n",
        "    3: \"284470004|63593006\", # premature atrial contraction (PAC) | supraventricular premature beats (SVPB)\n",
        "    4: \"427172004|17338001\", # premature ventricular contractions (PVC) | ventricular premature beats (VPB)\n",
        "    5: \"6374002\", # bundle branch block (BBB)\n",
        "    6: \"426627000\", # bradycardia (Brady)\n",
        "    7: \"733534002|164909002\", # complete left bundle branch block (CLBBB) | left bundle branch block (LBBB)\n",
        "    8: \"713427006|59118001\", # complete right bundle branch block (CRBBB) | right bundle branch block (RBBB)\n",
        "    9: \"270492004\", # 1st degree av block (IAVB)\n",
        "    10: \"713426002\", # incomplete right bundle branch block (IRBBB)\n",
        "    11: \"39732003\", # left axis deviation (LAD)\n",
        "    12: \"445118002\", # left anterior fascicular block (LAnFB)\n",
        "    13: \"251146004\", # low qrs voltages (LQRSV)\n",
        "    14: \"698252002\", # nonspecific intraventricular conduction disorder (NSIVCB)\n",
        "    15: \"10370003\", # pacing rhythm (PR)\n",
        "    16: \"365413008\", # poor R wave Progression (PRWP)\n",
        "    17: \"164947007\", # prolonged pr interval (LPR)\n",
        "    18: \"111975006\", # prolonged qt interval (LQT)\n",
        "    19: \"164917005\", # qwave abnormal (QAb)\n",
        "    20: \"47665007\",  # right axis deviation (RAD)\n",
        "    21: \"427393009\", # sinus arrhythmia (SA)\n",
        "    22: \"426177001\", # sinus bradycardia (SB)\n",
        "    23: \"427084000\", # sinus tachycardia (STach)\n",
        "    24: \"164934002\", # t wave abnormal (TAb)\n",
        "    25: \"59931005\" # t wave inversion (TInv)\n",
        "}\n",
        "\n",
        "def map_arrhyhtmia_index_to_id(x: int) -> str:\n",
        "    return arrhyhtmia_mapping_index_to_id[x]"
      ],
      "metadata": {
        "id": "g3IMoWUCkMc5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LZTY2UE1Mhlj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e03415b5-fd79-418e-84d2-7051557e37f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "codes_SNOMED.csv  physionet2017_references.csv\tphysionet2021_references.csv\n",
            "physionet2017.h5  physionet2021.h5\t\tprepared\n"
          ]
        }
      ],
      "source": [
        "# == Mount drive ==\n",
        "\n",
        "# https://drive.google.com/drive/folders/1L_gOMrkygu2N0k97COYuVrmE-AwEEMoQ\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/My Drive/Master Thesis/Datasets\"\n",
        "!ls \"/content/drive/My Drive/Master Thesis/Datasets\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejP3dsia8Wdq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4db831c-3339-457a-a4ba-093849aca232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Load ECG data:  67%|██████▋   | 54629/81960 [02:09<03:24, 133.67it/s]"
          ]
        }
      ],
      "source": [
        "# == Load all Physionet2021 ECGs and their IDs to a dictionary X_dict ==\n",
        "\n",
        "X_dict = {}\n",
        "Y_dict = {}\n",
        "\n",
        "h5file = h5py.File(os.path.join(path, \"prepared/physionet2021_scoredLabels.h5\"), \"r\")\n",
        "IDs = list(h5file.keys())\n",
        "pbar = tqdm(total=len(IDs), desc=\"Load ECG data\", position=0, leave=True)\n",
        "for key in IDs:\n",
        "    X_dict[key] = list(h5file[key][0])\n",
        "    pbar.update(1)\n",
        "\n",
        "# == Load all labels and their IDs to a dictionary Y_dict (some ECGs can have multiple labels) ==\n",
        "\n",
        "labels_df = pd.read_csv(os.path.join(path, \"physionet2021_references.csv\"), sep=\";\")\n",
        "pbar = tqdm(total=len(labels_df), desc=\"Load ECG labels\", position=0, leave=True)\n",
        "for _, row in labels_df.iterrows():\n",
        "    labels = row[\"labels\"].strip().split(\",\")\n",
        "    labels_valid = []\n",
        "    for label in labels:\n",
        "        if label in VALID_LABELS:\n",
        "              labels_valid.append(map_arrhyhtmia_id_to_index(label))\n",
        "    if row[\"id\"] in X_dict:\n",
        "        Y_dict[row[\"id\"]] = labels_valid\n",
        "    pbar.update(1)\n",
        "\n",
        "# del IDs, h5file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# == Preprocess ECGs ==\n",
        "\n",
        "pbar = tqdm(total=len(X_dict), desc=\"Preprocess ECGs\", position=0, leave=True)\n",
        "for key in X_dict:\n",
        "    X_dict[key] = pad_or_truncate_ecg(ecg=X_dict[key], max_samples=5000)\n",
        "    X_dict[key] = resample_ecg(ecg=X_dict[key], resample=2000)\n",
        "    X_dict[key] = normalize_to_minus11(ecg=X_dict[key])\n",
        "    X_dict[key] = butter_bandpass_filter(ecg=X_dict[key], lowcut=0.3, highcut=21.0, sampling_rate=200)\n",
        "    pbar.update(1)\n"
      ],
      "metadata": {
        "id": "yibThGeHx5U7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkTDLTJy9ku0"
      },
      "outputs": [],
      "source": [
        "# == Map scored labels to ECGs and create three lists (X: ECGs, Y: labels, Z: IDs) ==\n",
        "\n",
        "Y = []\n",
        "Z = []\n",
        "\n",
        "for patient_id in tqdm(Y_dict, desc=\"Map labels to ECGs\", position=0, leave=True):\n",
        "    for label in Y_dict[patient_id]:\n",
        "        Y.append(label)\n",
        "        Z.append(str(patient_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAYphezXWKSx"
      },
      "outputs": [],
      "source": [
        "# == Split train-test sets by patients if train-test splits files are given ==\n",
        "\n",
        "if os.path.exists(os.path.join(path, \"prepared/trainset_patient_ids.txt\")) and os.path.exists(os.path.join(path, \"prepared/testset_patient_ids.txt\")):\n",
        "    f = open(os.path.join(path, \"prepared/trainset_patient_ids.txt\"), \"r\", encoding=\"utf-8\")\n",
        "    trainset_patient_ids = f.readlines()\n",
        "    f.close()\n",
        "    f = open(os.path.join(path, \"prepared/testset_patient_ids.txt\"), \"r\", encoding=\"utf-8\")\n",
        "    testset_patient_ids = f.readlines()\n",
        "    f.close()\n",
        "    trainset_patient_ids = list(map(lambda x: x.replace(\"\\n\", \"\"), trainset_patient_ids))\n",
        "    testset_patient_ids = list(map(lambda x: x.replace(\"\\n\", \"\"), testset_patient_ids))\n",
        "    trainset_patient_ids = set(trainset_patient_ids)\n",
        "    testset_patient_ids = set(testset_patient_ids)\n",
        "    Y_train = []\n",
        "    Z_train = []\n",
        "    Y_test = []\n",
        "    Z_test = []\n",
        "    for sample in zip(Y, Z):\n",
        "        if sample[1] in trainset_patient_ids:\n",
        "            Y_train.append(sample[0])\n",
        "            Z_train.append(sample[1])\n",
        "        elif sample[1] in testset_patient_ids:\n",
        "            Y_test.append(sample[0])\n",
        "            Z_test.append(sample[1])\n",
        "    # del Y, Z\n",
        "else:\n",
        "    print(\"No predefined train-test split files are found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1t3m5cUio4c"
      },
      "outputs": [],
      "source": [
        "# == Split data train-test sets by patients ==\n",
        "\n",
        "if not os.path.exists(os.path.join(path, \"prepared/trainset_patient_ids.txt\")) or not os.path.exists(os.path.join(path, \"prepared/testset_patient_ids.txt\")):\n",
        "    bins = []\n",
        "    for x in range(NUM_CLASSES):\n",
        "        bins.append([])\n",
        "\n",
        "    # Create num_classes bins (1 bin = 1 class)\n",
        "    for sample in zip(Y, Z):\n",
        "        bins[sample[0]].append([sample[0], sample[1]])\n",
        "\n",
        "    # Create train-test bins (e.g. TRAIN_TEST_SPLIT = 80%-20%)\n",
        "    train_bins = []\n",
        "    test_bins = []\n",
        "    for x in range(NUM_CLASSES):\n",
        "        train_bins.append([])\n",
        "        test_bins.append([])\n",
        "    for index, bin in enumerate(bins):\n",
        "        split_index = int(len(bin) * TRAIN_TEST_SPLIT)\n",
        "        train_bins[index] = bin[:split_index]\n",
        "        test_bins[index] = bin[split_index:]\n",
        "\n",
        "    # Create test set (sort bins by occurence of labels descending before adding to train set because of minor classes and multiple labels and add if patient id is not in train set)\n",
        "    Y_test = []\n",
        "    Z_test = []\n",
        "    id_already_in_x_test = set()\n",
        "    test_bins.sort(key=lambda x: len(x))\n",
        "    for bin in test_bins:\n",
        "        for sample in bin:\n",
        "            id_already_in_x_test.add(sample[1])\n",
        "            Y_test.append(sample[0])\n",
        "            Z_test.append(sample[1])\n",
        "\n",
        "    # Create train set (sort bins by occurence of labels descending before adding to train set because of minor classes and multiple labels and add if patient id is not in train (split by patients) or test set (multiple labels in test set would bias the evaluation) already)\n",
        "    Y_train = []\n",
        "    Z_train = []\n",
        "    id_already_in_x_train = set()\n",
        "    train_bins.sort(key=lambda x: len(x))\n",
        "    for bin in train_bins:\n",
        "        for sample in bin:\n",
        "            if sample[1] not in id_already_in_x_test:\n",
        "                id_already_in_x_train.add(sample[1])\n",
        "                Y_train.append(sample[0])\n",
        "                Z_train.append(sample[1])\n",
        "\n",
        "    # Write train and test patient ids to file\n",
        "    id_already_in_x_train_list = list(id_already_in_x_train)\n",
        "    id_already_in_x_train_list = list(map(lambda x: str(x) + \"\\n\", id_already_in_x_train_list))\n",
        "    id_already_in_x_test_list = list(id_already_in_x_test)\n",
        "    id_already_in_x_test_list = list(map(lambda x: str(x) + \"\\n\", id_already_in_x_test_list))\n",
        "    # with open('trainset_patient_ids.txt', 'w', encoding='utf-8') as file:\n",
        "    #    file.writelines(id_already_in_x_train_list)\n",
        "    # with open('testset_patient_ids.txt', 'w', encoding='utf-8') as file:\n",
        "    #    file.writelines(id_already_in_x_test_list)\n",
        "    # del Y, Z\n",
        "else:\n",
        "    print(\"Train-test split files are present, no new split was created!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for index, z in enumerate(Z_train):\n",
        "    Z_train[index] = [Z_train[index]]\n",
        "for index, z in enumerate(Z_test):\n",
        "    Z_test[index] = [Z_test[index]]"
      ],
      "metadata": {
        "id": "fvOmmKpwKubi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejt6gOg7dPMd"
      },
      "outputs": [],
      "source": [
        "# == Balance data ==\n",
        "\n",
        "# Trainset\n",
        "\n",
        "count_train = Counter(Y_train)\n",
        "\n",
        "undersampling_strategy = {}\n",
        "oversampling_strategy = {}\n",
        "for i in count_train:\n",
        "    if count_train[i] > CLASS_BALANCE:\n",
        "        undersampling_strategy[i] = CLASS_BALANCE\n",
        "    elif count_train[i] <= CLASS_BALANCE:\n",
        "        oversampling_strategy[i] = CLASS_BALANCE\n",
        "print(f\"Trainset undersampling_strategy: {undersampling_strategy}\")\n",
        "print(f\"Trainset oversampling_strategy: {oversampling_strategy}\")\n",
        "\n",
        "under = RandomUnderSampler(sampling_strategy=undersampling_strategy)\n",
        "over = RandomOverSampler(sampling_strategy=oversampling_strategy)\n",
        "steps = [(\"u\", under), (\"o\", over)]\n",
        "pipeline = Pipeline(steps=steps)\n",
        "Z_train_balanced, Y_train_balanced = pipeline.fit_resample(Z_train, Y_train)\n",
        "\n",
        "count_train_balanced = Counter(Y_train_balanced)\n",
        "\n",
        "# Testset\n",
        "\n",
        "count_test = Counter(Y_test)\n",
        "\n",
        "# min_key = min(count_test, key=count_test.get)\n",
        "# min_value = count_test[min_key]\n",
        "\n",
        "undersampling_strategy = {}\n",
        "for i in count_test:\n",
        "    if count_test[i] > TEST_BALANCE:\n",
        "        undersampling_strategy[i] = TEST_BALANCE\n",
        "\n",
        "print(f\"Testset undersampling_strategy: {undersampling_strategy}\")\n",
        "\n",
        "under = RandomUnderSampler(sampling_strategy=undersampling_strategy)\n",
        "steps = [(\"u\", under)]\n",
        "pipeline = Pipeline(steps=steps)\n",
        "Z_test_balanced, Y_test_balanced = pipeline.fit_resample(Z_test, Y_test)\n",
        "\n",
        "count_test_balanced = Counter(Y_test_balanced)\n",
        "\n",
        "print(f\"Trainset: {count_train}\")\n",
        "print(f\"Testset: {count_test}\")\n",
        "print(f\"Trainset balanced: {count_train_balanced}\")\n",
        "print(f\"Testset balanced: {count_test_balanced}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for index, z in enumerate(Z_train):\n",
        "    Z_train[index] = Z_train[index][0]\n",
        "for index, z in enumerate(Z_test):\n",
        "    Z_test[index] = Z_test[index][0]\n",
        "for index, z in enumerate(Z_train_balanced):\n",
        "    Z_train_balanced[index] = Z_train_balanced[index][0]\n",
        "for index, z in enumerate(Z_test_balanced):\n",
        "    Z_test_balanced[index] = Z_test_balanced[index][0]"
      ],
      "metadata": {
        "id": "bt8OyYkZNkXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# == Create binary_crossentropy X_train, Y_train, X_train_balanced, Y_train_balanced, X_test, Y_test, X_test_balanced, Y_test_balanced ==\n",
        "\n",
        "for key in Y_dict:\n",
        "    binary_crossentropy_labels = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "    for label in Y_dict[key]:\n",
        "        binary_crossentropy_labels[label] = 1\n",
        "    Y_dict[key] = binary_crossentropy_labels\n",
        "\n",
        "X_train = []\n",
        "Y_train = []\n",
        "Z_train_new = []\n",
        "X_train_balanced = []\n",
        "Y_train_balanced = []\n",
        "Z_train_balanced_new = []\n",
        "X_test = []\n",
        "Y_test = []\n",
        "Z_test_new = []\n",
        "X_test_balanced = []\n",
        "Y_test_balanced = []\n",
        "Z_test_balanced_new = []\n",
        "\n",
        "for patient_id in set(Z_train):\n",
        "    X_train.append(X_dict[patient_id])\n",
        "    Y_train.append(Y_dict[patient_id])\n",
        "    Z_train_new.append(patient_id)\n",
        "\n",
        "for patient_id in set(Z_train_balanced):\n",
        "    X_train_balanced.append(X_dict[patient_id])\n",
        "    Y_train_balanced.append(Y_dict[patient_id])\n",
        "    Z_train_balanced_new.append(patient_id)\n",
        "\n",
        "for patient_id in set(Z_test):\n",
        "    X_test.append(X_dict[patient_id])\n",
        "    Y_test.append(Y_dict[patient_id])\n",
        "    Z_test_new.append(patient_id)\n",
        "\n",
        "for patient_id in set(Z_test_balanced):\n",
        "    X_test_balanced.append(X_dict[patient_id])\n",
        "    Y_test_balanced.append(Y_dict[patient_id])\n",
        "    Z_test_balanced_new.append(patient_id)\n",
        "\n",
        "Z_train = Z_train_new\n",
        "Z_train_balanced = Z_train_balanced_new\n",
        "Z_test = Z_test_new\n",
        "Z_test_balanced_new = Z_test_balanced_new"
      ],
      "metadata": {
        "id": "GPX3RLJoosEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oH1on-V7RJ1s"
      },
      "outputs": [],
      "source": [
        "# == Shuffle data, convert to numpy lists and reshape ==\n",
        "\n",
        "# Shuffle data\n",
        "\n",
        "combined = list(zip(X_train, Y_train, Z_train))\n",
        "random.shuffle(combined)\n",
        "X_train, Y_train, Z_train = zip(*combined)\n",
        "X_train = list(X_train)\n",
        "Y_train = list(Y_train)\n",
        "Z_train = list(Z_train)\n",
        "\n",
        "combined = list(zip(X_test, Y_test, Z_test))\n",
        "random.shuffle(combined)\n",
        "X_test, Y_test, Z_test = zip(*combined)\n",
        "X_test = list(X_test)\n",
        "Y_test = list(Y_test)\n",
        "Z_test = list(Z_test)\n",
        "\n",
        "combined = list(zip(X_train_balanced, Y_train_balanced, Z_train_balanced))\n",
        "random.shuffle(combined)\n",
        "X_train_balanced, Y_train_balanced, Z_train_balanced = zip(*combined)\n",
        "X_train_balanced = list(X_train_balanced)\n",
        "Y_train_balanced = list(Y_train_balanced)\n",
        "Z_train_balanced = list(Z_train_balanced)\n",
        "\n",
        "combined = list(zip(X_test_balanced, Y_test_balanced, Z_test_balanced))\n",
        "random.shuffle(combined)\n",
        "X_test_balanced, Y_test_balanced, Z_test_balanced = zip(*combined)\n",
        "X_test_balanced = list(X_test_balanced)\n",
        "Y_test_balanced = list(Y_test_balanced)\n",
        "Z_test_balanced = list(Z_test_balanced)\n",
        "\n",
        "print(f\"Y_train: {Y_train[:50]}\")\n",
        "print(f\"Y_test: {Y_test[:50]}\")\n",
        "print(f\"Y_train_balanced: {Y_train_balanced[:50]}\")\n",
        "print(f\"Y_test_balanced: {Y_test_balanced[:50]}\")\n",
        "\n",
        "# Convert to numpy lists\n",
        "\n",
        "# Y_train = np.array(Y_train)\n",
        "# Y_test = np.array(Y_test)\n",
        "# Y_train_balanced = np.array(Y_train_balanced)\n",
        "# Y_test_balanced = np.array(Y_test_balanced)\n",
        "\n",
        "for index, x in enumerate(Y_train):\n",
        "    Y_train[index] = np.array(x)\n",
        "Y_train = np.array(Y_train)\n",
        "\n",
        "for index, x in enumerate(Y_test):\n",
        "    Y_test[index] = np.array(x)\n",
        "Y_test = np.array(Y_test)\n",
        "\n",
        "for index, x in enumerate(Y_train_balanced):\n",
        "    Y_train_balanced[index] = np.array(x)\n",
        "Y_train_balanced = np.array(Y_train_balanced)\n",
        "\n",
        "for index, x in enumerate(Y_test_balanced):\n",
        "    Y_test_balanced[index] = np.array(x)\n",
        "Y_test_balanced = np.array(Y_test_balanced)\n",
        "\n",
        "for index, x in enumerate(X_train):\n",
        "    X_train[index] = np.array(x)\n",
        "X_train = np.array(X_train)\n",
        "\n",
        "for index, x in enumerate(X_test):\n",
        "    X_test[index] = np.array(x)\n",
        "X_test = np.array(X_test)\n",
        "\n",
        "for index, x in enumerate(X_train_balanced):\n",
        "    X_train_balanced[index] = np.array(x)\n",
        "X_train_balanced = np.array(X_train_balanced)\n",
        "\n",
        "for index, x in enumerate(X_test_balanced):\n",
        "    X_test_balanced[index] = np.array(x)\n",
        "X_test_balanced = np.array(X_test_balanced)\n",
        "\n",
        "# Reshape\n",
        "\n",
        "X_train = X_train.reshape((-1, 2000, 1))\n",
        "X_test = X_test.reshape((-1, 2000, 1))\n",
        "X_train_balanced = X_train_balanced.reshape((-1, 2000, 1))\n",
        "X_test_balanced = X_test_balanced.reshape((-1, 2000, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEsV9r0EjSTw"
      },
      "outputs": [],
      "source": [
        "# == A-B-testing models ==\n",
        "\n",
        "# Model A: Residual_CNN_1lead\n",
        "def conv(i, filters=16, kernel_size=3, strides=1):\n",
        "    i = keras.layers.Conv1D(\n",
        "        filters=filters, kernel_size=kernel_size, strides=strides, padding=\"same\"\n",
        "    )(i)\n",
        "    i = keras.layers.BatchNormalization()(i)\n",
        "    i = keras.layers.LeakyReLU()(i)\n",
        "    i = keras.layers.SpatialDropout1D(0.1)(i)\n",
        "    return i\n",
        "def residual_unit(x, filters, layers=3):\n",
        "    inp = x\n",
        "    for i in range(layers):\n",
        "        x = conv(x, filters)\n",
        "    return keras.layers.add([x, inp])\n",
        "def conv_block(x, filters, strides):\n",
        "    x = conv(x, filters)\n",
        "    x = residual_unit(x, filters)\n",
        "    if strides > 1:\n",
        "        x = keras.layers.AveragePooling1D(strides, strides)(x)\n",
        "    return x\n",
        "def build_model_A(input_shape, num_classes):\n",
        "    inp = keras.layers.Input(input_shape)\n",
        "    x = inp\n",
        "    x = conv_block(x, 16, 2)\n",
        "    x = conv_block(x, 32, 2)\n",
        "    x = conv_block(x, 64, 2)\n",
        "    x = conv_block(x, 128, 2)\n",
        "    # x = keras.layers.Masking(mask_value=0)(x)\n",
        "    # x = keras.layers.GRU(128, recurrent_dropout=0.1)(x)\n",
        "    x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "    x = keras.layers.Dense(num_classes, activation=\"sigmoid\")(x)\n",
        "    model = keras.models.Model(inp, x)\n",
        "    return model\n",
        "\n",
        "# Model B: CNN_Transformer_1lead\n",
        "def build_model_B(num_classes, input_shape):\n",
        "    # input_shape = (2000, 1)  # Each sample has 2000 timesteps and 1 feature per timestep\n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "    # Masking for padded/truncated data\n",
        "    # i = keras.layers.Masking(mask_value=0)(input_layer)\n",
        "    # Conv1\n",
        "    i = keras.layers.Conv1D(filters=128, kernel_size=3, strides=1, padding=\"same\")(input_layer)\n",
        "    i = keras.layers.BatchNormalization()(i)\n",
        "    i = keras.layers.ReLU()(i)\n",
        "    i = keras.layers.SpatialDropout1D(0.1)(i)\n",
        "    i = keras.layers.AveragePooling1D(2)(i)\n",
        "    # Conv2\n",
        "    i = keras.layers.Conv1D(filters=256, kernel_size=3, strides=1, padding=\"same\")(i)\n",
        "    i = keras.layers.BatchNormalization()(i)\n",
        "    i = keras.layers.ReLU()(i)\n",
        "    i = keras.layers.SpatialDropout1D(0.1)(i)\n",
        "    i = keras.layers.AveragePooling1D(2)(i)\n",
        "    # Conv3\n",
        "    i = keras.layers.Conv1D(filters=512, kernel_size=3, strides=1, padding=\"same\")(i)\n",
        "    i = keras.layers.BatchNormalization()(i)\n",
        "    i = keras.layers.ReLU()(i)\n",
        "    i = keras.layers.SpatialDropout1D(0.1)(i)\n",
        "    i = keras.layers.AveragePooling1D(2)(i)\n",
        "    # Conv3\n",
        "    i = keras.layers.Conv1D(filters=1024, kernel_size=3, strides=1, padding=\"same\")(i)\n",
        "    i = keras.layers.BatchNormalization()(i)\n",
        "    i = keras.layers.ReLU()(i)\n",
        "    i = keras.layers.SpatialDropout1D(0.1)(i)\n",
        "    i = keras.layers.AveragePooling1D(2)(i)\n",
        "    # Channel Average Pooling and Reshaping\n",
        "    i = keras.layers.GlobalAveragePooling1D(data_format=\"channels_last\")(i)\n",
        "    i = keras.layers.Reshape((8, 128))(i)\n",
        "    # Encoder block/Attention mechanisms\n",
        "    i = transformer_encoder(i, input_shape=(8, 128), key_dim=8, num_heads=8, ff_dim=200, dropout=0.1)\n",
        "    i = transformer_encoder(i, input_shape=(8, 128), key_dim=8, num_heads=8, ff_dim=200, dropout=0.1)\n",
        "    i = transformer_encoder(i, input_shape=(8, 128), key_dim=8, num_heads=8, ff_dim=200, dropout=0.1)\n",
        "    # Flatten\n",
        "    i = keras.layers.Flatten()(i)\n",
        "    # Feedforward Softmax\n",
        "    i = keras.layers.Dense(num_classes, activation=\"sigmoid\")(i)\n",
        "    return keras.models.Model(inputs=input_layer, outputs=i)\n",
        "\n",
        "# Model C: vanilla_Transformer_1lead\n",
        "def transformer_encoder(input, input_shape, num_heads, key_dim, ff_dim, dropout):\n",
        "    # Multi-Head Attention\n",
        "    x = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, dropout=dropout, kernel_regularizer=regularizers.l2(0.001))(input, input)\n",
        "    # Add & Normalize\n",
        "    res = x + input\n",
        "    x = keras.layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    # Feed-Forward Layer\n",
        "    x = keras.layers.Flatten(input_shape=input_shape)(x)\n",
        "    x = keras.layers.Dense(units=ff_dim, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = keras.layers.Dense(input_shape[0] * input_shape[1], kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = keras.layers.Reshape(input_shape)(x)\n",
        "    x = keras.layers.Dropout(rate=dropout)(x)\n",
        "    # Add & Normalize\n",
        "    x = x + res\n",
        "    x = keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    return x\n",
        "\n",
        "def build_model_C(num_classes, input_shape, num_encoder_blocks, num_heads, key_dim, ff_dim, dropout):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_encoder_blocks):\n",
        "        x = transformer_encoder(x, input_shape, key_dim, num_heads, ff_dim, dropout)\n",
        "    x = keras.layers.Flatten(input_shape=input_shape)(x)\n",
        "    outputs = keras.layers.Dense(num_classes, activation='sigmoid')(x)\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "# Model D: channel_attention_1lead\n",
        "from tensorflow.keras.layers import Input, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D, Dense, Multiply, Add, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def channel_attention(input_feature, ratio=8):\n",
        "    channel = input_feature.shape[-1]\n",
        "\n",
        "    shared_layer_one = Dense(channel//ratio, activation='relu', kernel_initializer='he_normal', use_bias=True, bias_initializer='zeros')\n",
        "    shared_layer_two = Dense(channel, kernel_initializer='he_normal', use_bias=True, bias_initializer='zeros')\n",
        "\n",
        "    avg_pool = GlobalAveragePooling1D()(input_feature)\n",
        "    avg_pool = shared_layer_one(avg_pool)\n",
        "    avg_pool = shared_layer_two(avg_pool)\n",
        "\n",
        "    max_pool = GlobalMaxPooling1D()(input_feature)\n",
        "    max_pool = shared_layer_one(max_pool)\n",
        "    max_pool = shared_layer_two(max_pool)\n",
        "\n",
        "    cbam_feature = Add()([avg_pool, max_pool])\n",
        "    cbam_feature = Activation('sigmoid')(cbam_feature)\n",
        "\n",
        "    return Multiply()([input_feature, cbam_feature])\n",
        "\n",
        "def build_model_D(input_shape, num_classes):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    x = Conv1D(64, kernel_size=3, padding='same', activation='relu')(inputs)\n",
        "    x = channel_attention(x)\n",
        "\n",
        "    x = Conv1D(128, kernel_size=3, padding='same', activation='relu')(x)\n",
        "    x = channel_attention(x)\n",
        "\n",
        "    x = Conv1D(256, kernel_size=3, padding='same', activation='relu')(x)\n",
        "    x = channel_attention(x)\n",
        "\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conv(i, filters=16, kernel_size=3, strides=1):\n",
        "    i = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding=\"same\")(i)\n",
        "    i = keras.layers.BatchNormalization()(i)\n",
        "    i = keras.layers.LeakyReLU()(i)\n",
        "    i = keras.layers.SpatialDropout1D(0.1)(i)\n",
        "    return i\n",
        "\n",
        "def inception(inputs, filters):\n",
        "    conv1 = conv(inputs, filters, kernel_size=3)\n",
        "    conv2 = conv(inputs, filters, kernel_size=5)\n",
        "    # conv3 = conv(inputs, filters, kernel_size=7)\n",
        "    conv4 = conv(inputs, filters, kernel_size=9)\n",
        "    # conv5 = conv(inputs, filters, kernel_size=11)\n",
        "    # conv6 = conv(inputs, filters, kernel_size=13)\n",
        "    conv7 = conv(inputs, filters, kernel_size=15)\n",
        "    # concatenated = keras.layers.Concatenate()([conv1, conv2, conv3, conv4, conv5, conv6, conv7])\n",
        "    concatenated = keras.layers.Concatenate()([conv1, conv2, conv4, conv7])\n",
        "    return concatenated\n",
        "\n",
        "def residual_unit(x, filters, layers=1):\n",
        "    inp = x\n",
        "    for i in range(layers):\n",
        "        x = inception(x, filters)\n",
        "        # x = conv(x, filters)\n",
        "    return keras.layers.add([x, inp])\n",
        "\n",
        "def conv_block(x, filters, strides):\n",
        "    x = conv(x, filters*4)\n",
        "    x = residual_unit(x, filters)\n",
        "    if strides > 1:\n",
        "        x = keras.layers.MaxPooling1D(strides, strides)(x)\n",
        "    return x\n",
        "\n",
        "def build_model(input_shape, num_classes):\n",
        "    inp = keras.layers.Input(input_shape)\n",
        "    x = inp\n",
        "    x = conv_block(x, 16, 2)\n",
        "    x = conv_block(x, 32, 2)\n",
        "    # x = conv_block(x, 64, 2)\n",
        "    # x = conv_block(x, 128, 2)\n",
        "    x = keras.layers.GlobalMaxPooling1D()(x)\n",
        "    x = keras.layers.Dense(num_classes, activation=\"sigmoid\")(x)\n",
        "    model = keras.models.Model(inp, x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "x8ikm1DkblnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yIfrPc8R27b"
      },
      "outputs": [],
      "source": [
        "# == Initialize model ==\n",
        "\n",
        "# Explicitly specify the GPU device\n",
        "physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
        "if len(physical_devices) > 0:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "input_shape = X_train_balanced.shape\n",
        "num_encoder_blocks = 8\n",
        "num_heads = 8 # 8\n",
        "key_dim = 25 # 25\n",
        "ff_dim = 24 # 24\n",
        "dropout = 0.1\n",
        "\n",
        "# Reshape\n",
        "X_train = X_train.reshape(-1, 2000, 1) # (-1, 2000, 1) (-1, 10, 200) (-1, 40, 50)\n",
        "X_test = X_test.reshape(-1, 2000, 1)\n",
        "X_train_balanced = X_train_balanced.reshape(-1, 2000, 1)\n",
        "X_test_balanced = X_test_balanced.reshape(-1, 2000, 1)\n",
        "\n",
        "# model = build_model_D(num_classes=NUM_CLASSES, input_shape=input_shape[1:], num_encoder_blocks=num_encoder_blocks, num_heads=num_heads, key_dim=key_dim, ff_dim=ff_dim, dropout=dropout)\n",
        "model = build_model(num_classes=NUM_CLASSES, input_shape=input_shape[1:])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the class weights from the uploaded file\n",
        "weights_path = os.path.join(path, \"prepared/Physionet2021_evaluation/weights.csv\")\n",
        "class_weights_df = pd.read_csv(weights_path)\n",
        "\n",
        "# Reorder rows according to new_order\n",
        "new_order = [1, 2, 5, 6, 7, 8, 9, 10, 11, 12, 17, 13, 18, 14, 0, 3, 15, 16, 4, 19, 20, 21, 22, 23, 24, 25]\n",
        "class_weights_df = class_weights_df.iloc[new_order].reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Set the index and convert to numpy array\n",
        "class_weights = class_weights_df.set_index(class_weights_df.columns[0]).values\n",
        "\n",
        "# Calculate the inverse of class weights\n",
        "inverse_class_weights = 1 / class_weights\n",
        "\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_true, y_pred))\n",
        "\n",
        "def sparsity_loss(y_pred):\n",
        "    return -4 * y_pred * (y_pred - 1)\n",
        "\n",
        "def challenge_loss(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "\n",
        "    # Compute confusion matrix entries\n",
        "    a_ij = tf.matmul(tf.expand_dims(y_true, -1), tf.expand_dims(y_pred, 1))\n",
        "\n",
        "    # Compute weighted sum\n",
        "    cl_loss = tf.reduce_sum(class_weights * a_ij)\n",
        "    return cl_loss\n",
        "\n",
        "# Custom loss function\n",
        "def custom_loss(y_true, y_pred):\n",
        "    bce = binary_cross_entropy(y_true, y_pred)\n",
        "    sl = sparsity_loss(y_pred)\n",
        "    cl = challenge_loss(y_true, y_pred)\n",
        "\n",
        "    return bce - cl + sl"
      ],
      "metadata": {
        "id": "xm9gknvrGRtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3jbMs35jnOL"
      },
      "outputs": [],
      "source": [
        "# == Train model ==\n",
        "\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Check if a GPU is available\n",
        "print(\"Number of GPUs available:\", len(tf.config.experimental.list_physical_devices(\"GPU\")))\n",
        "print(f\"Number of training data examples: {len(X_train_balanced)}\")\n",
        "print(f\"Number of classes: {NUM_CLASSES}\")\n",
        "print(f\"Shape of training data: {input_shape}\")\n",
        "print(f\"Epochs: {EPOCHS}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(f\"Model_{NUM_CLASSES}classes.h5\", save_best_only=True, monitor=\"val_loss\"),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5, min_lr=0.0000001),\n",
        "    # reduce_lr,\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, verbose=1),\n",
        "]\n",
        "\n",
        "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
        "# optimizer = Adam(learning_rate=0.01)\n",
        "\n",
        "\n",
        "# loss=\"binary_crossentropy\", metrics=[\"binary_accuracy\"]; sparse_categorical_crossentropy, metrics=[\"sparse_categorical_accuracy\"]\n",
        "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]) # \"binary_crossentropy\" custom_loss\n",
        "history = model.fit(X_train_balanced, Y_train_balanced, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=callbacks, validation_split=0.25, verbose=1)\n",
        "\n",
        "train_accuracy = history.history[\"accuracy\"] # list\n",
        "val_accuracy = history.history[\"val_accuracy\"] # list\n",
        "train_loss = history.history[\"loss\"] # list\n",
        "val_loss = history.history[\"val_loss\"] # list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_prob = model.predict(X_train)"
      ],
      "metadata": {
        "id": "5ALeoxbYEOHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_prob[1]\n",
        "for i in pred_prob[1]:\n",
        "    print(float(i))\n",
        "print(Y_train[1])"
      ],
      "metadata": {
        "id": "iw60OIvkGE_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the top layer\n",
        "top_input = keras.Input(shape=(NUM_CLASSES,))\n",
        "top_output = keras.layers.Dense(NUM_CLASSES, activation='sigmoid')(top_input)\n",
        "top_model = Model(inputs=top_input, outputs=top_output)"
      ],
      "metadata": {
        "id": "ko40nLi3GD97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the class weights from the uploaded file\n",
        "weights_path = os.path.join(path, \"prepared/Physionet2021_evaluation/weights.csv\")\n",
        "class_weights_df = pd.read_csv(weights_path)\n",
        "\n",
        "# Reorder rows according to new_order\n",
        "new_order = [1, 2, 5, 6, 7, 8, 9, 10, 11, 12, 17, 13, 18, 14, 0, 3, 15, 16, 4, 19, 20, 21, 22, 23, 24, 25]\n",
        "class_weights_df = class_weights_df.iloc[new_order].reset_index(drop=True)\n",
        "# Set the index and convert to numpy array\n",
        "class_weights = class_weights_df.set_index(class_weights_df.columns[0]).values\n",
        "# Calculate the inverse of class weights\n",
        "inverse_class_weights = 1 / class_weights\n",
        "\n",
        "def sparsity_loss(y_pred):\n",
        "    return -4 * y_pred * (y_pred - 1)\n",
        "\n",
        "def challenge_loss(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "\n",
        "    # Compute confusion matrix entries\n",
        "    a_ij = tf.matmul(tf.expand_dims(y_true, -1), tf.expand_dims(y_pred, 1))\n",
        "\n",
        "    # Compute weighted sum\n",
        "    cl_loss = tf.reduce_sum(class_weights * a_ij)\n",
        "    return cl_loss\n",
        "\n",
        "# Custom loss function\n",
        "def custom_loss(y_true, y_pred):\n",
        "    sl = sparsity_loss(y_pred)\n",
        "    cl = challenge_loss(y_true, y_pred)\n",
        "\n",
        "    return cl + sl"
      ],
      "metadata": {
        "id": "zGD2OUCuGzkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the top model with custom loss\n",
        "top_model.compile(optimizer='adam', loss=custom_loss)\n",
        "# Train the top model using the output of the main model as input\n",
        "top_model.fit(pred_prob, Y_train, epochs=5)"
      ],
      "metadata": {
        "id": "7PqMo1_uGLtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get input of model A\n",
        "inputs = model.input\n",
        "\n",
        "# Use output from model A as input to model B\n",
        "output_A = model(inputs)\n",
        "output_B = top_model(output_A)\n",
        "\n",
        "# Create a new model that links input from A to output of B\n",
        "combined_model = tf.keras.Model(inputs=inputs, outputs=output_B)"
      ],
      "metadata": {
        "id": "frwYLqC1JgNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_prob2 = combined_model.predict(X_train)"
      ],
      "metadata": {
        "id": "R9BkX2vmNyXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_prob2[1]\n",
        "for i in pred_prob2[1]:\n",
        "    print(float(i))\n",
        "print(Y_train[1])"
      ],
      "metadata": {
        "id": "_KpBMBCVN3mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_prob3 = combined_model.predict(X_test)"
      ],
      "metadata": {
        "id": "hQd8G09zieOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "# Assuming 'predictions' from combined_model.predict(x_val) and 'y_val' are your validation labels\n",
        "num_classes = 26  # Number of classes\n",
        "thresholds = np.linspace(0, 1, 100)\n",
        "optimal_thresholds = np.zeros(num_classes)\n",
        "\n",
        "for i in range(num_classes):\n",
        "    f1_scores = []\n",
        "    for thresh in thresholds:\n",
        "        # Apply threshold to predictions for class 'i'\n",
        "        binary_predictions = (pred_prob[:, i] > thresh).astype(int)\n",
        "        # Calculate F1 score\n",
        "        f1 = f1_score(Y_test[:, i], binary_predictions)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    # Find the threshold that maximized the F1 score for this class\n",
        "    optimal_index = np.argmax(f1_scores)\n",
        "    optimal_thresholds[i] = thresholds[optimal_index]\n",
        "    print(f\"Class {i} optimal threshold: {thresholds[optimal_index]} with F1 Score: {f1_scores[optimal_index]}\")"
      ],
      "metadata": {
        "id": "h-Tg9c51hc4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rtmwe0Z-qO2n"
      },
      "outputs": [],
      "source": [
        "# == Plot training curve ==\n",
        "\n",
        "# Accuracy\n",
        "\n",
        "train_accuracy = train_accuracy\n",
        "val_accuracy = val_accuracy\n",
        "epochs_range = range(1, len(train_accuracy) + 1)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(epochs_range, train_accuracy, \"r\", label=\"Training Accuracy\")\n",
        "plt.plot(epochs_range, val_accuracy, \"b\", label=\"Validation Accuracy\")\n",
        "plt.title(\"Training and Validation Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.savefig(\n",
        "    \"Training_accuracy_\"+str(NUM_CLASSES)+\"classes.png\",\n",
        "    dpi=300,\n",
        "    format=\"png\",\n",
        "    bbox_inches=\"tight\",\n",
        ")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "# Loss\n",
        "\n",
        "train_loss = train_loss\n",
        "val_loss = val_loss\n",
        "epochs_range = range(1, len(train_loss) + 1)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(epochs_range, train_loss, \"r\", label=\"Training Loss\")\n",
        "plt.plot(epochs_range, val_loss, \"b\", label=\"Validation Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.savefig(\n",
        "    \"Training_loss_\"+str(NUM_CLASSES)+\"classes.png\",\n",
        "    dpi=300,\n",
        "    format=\"png\",\n",
        "    bbox_inches=\"tight\",\n",
        ")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Predict probabilities\n",
        "pred_prob = model.predict(X_test)\n",
        "\n",
        "# Binarize predictions using a threshold\n",
        "threshold = 0.5\n",
        "pred = (pred_prob > threshold).astype(int)\n",
        "\n",
        "# Calculate accuracy and other metrics\n",
        "accuracy = accuracy_score(Y_test, pred)\n",
        "precision = precision_score(Y_test, pred, average='weighted')  # or 'macro'\n",
        "recall = recall_score(Y_test, pred, average='weighted')\n",
        "f1 = f1_score(Y_test, pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy test set: {accuracy}\")\n",
        "print(f\"Precision test set: {precision}\")\n",
        "print(f\"Recall test set: {recall}\")\n",
        "print(f\"F1 Score test set: {f1}\")"
      ],
      "metadata": {
        "id": "cepu01n_JRn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# == Convert predictions unbalanced in Physionet\n",
        "import shutil\n",
        "\n",
        "try:\n",
        "    shutil.rmtree(\"test_outputs\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Directory not found.\")\n",
        "except PermissionError:\n",
        "    print(f\"Error: Permission denied.\")\n",
        "try:\n",
        "    os.makedirs(\"test_outputs\")\n",
        "except OSError as e:\n",
        "    print(f\"Error: {e.strerror}\")\n",
        "\n",
        "pbar = tqdm(total=len(pred), desc=\"Convert test_outputs\", position=0, leave=True)\n",
        "for index, prediction in enumerate(tqdm(zip(pred, pred_prob))):\n",
        "    pbar.update(1)\n",
        "    new_file = \"#\"\n",
        "    new_file += Z_test[index] + \"\\n\"\n",
        "    # ids\n",
        "    for pred_index, _ in enumerate(prediction[0]):\n",
        "        new_file += map_arrhyhtmia_index_to_id(pred_index) + \",\"\n",
        "    new_file = new_file[:-1] + \"\\n\"\n",
        "    # pred\n",
        "    for pred_index, _ in enumerate(prediction[0]):\n",
        "        if prediction[0][pred_index] == 1:\n",
        "            value = \"True\"\n",
        "        elif prediction[0][pred_index] == 0:\n",
        "            value = \"False\"\n",
        "        new_file += value + \",\"\n",
        "    new_file = new_file[:-1] + \"\\n\"\n",
        "    # pred_prob\n",
        "    for pred_index, _ in enumerate(prediction[1]):\n",
        "        new_file += str(prediction[1][pred_index]) + \",\"\n",
        "    new_file = new_file[:-1]\n",
        "    # with open(os.path.join(path, f\"test_outputs/{Z_test[index]}.csv\"), \"w\", encoding=\"utf-8\") as file:\n",
        "    #    file.write(new_file)\n",
        "    with open(f\"test_outputs/{Z_test[index]}.csv\", \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(new_file)"
      ],
      "metadata": {
        "id": "MJohDGH0ZwDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "folder_to_zip = \"test_outputs\"\n",
        "output_filename = 'test_outputs.zip'\n",
        "shutil.make_archive(output_filename.replace('.zip', ''), 'zip', folder_to_zip)\n",
        "files.download(output_filename)"
      ],
      "metadata": {
        "id": "sTng6qe6bIZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# == Apply on testset balanced ==\n",
        "\n",
        "# Predict probabilities\n",
        "pred_prob = model.predict(X_test_balanced)\n",
        "\n",
        "# Binarize predictions using a threshold\n",
        "threshold = 0.5\n",
        "pred = (pred_prob > threshold).astype(int)\n",
        "\n",
        "# Calculate accuracy and other metrics\n",
        "accuracy = accuracy_score(Y_test_balanced, pred)\n",
        "precision = precision_score(Y_test_balanced, pred, average='micro')  # or 'macro'\n",
        "recall = recall_score(Y_test_balanced, pred, average='micro')\n",
        "f1 = f1_score(Y_test_balanced, pred, average='micro')\n",
        "\n",
        "print(f\"Accuracy test set: {accuracy}\")\n",
        "print(f\"Precision test set: {precision}\")\n",
        "print(f\"Recall test set: {recall}\")\n",
        "print(f\"F1 Score test set: {f1}\")"
      ],
      "metadata": {
        "id": "bKa8poZGu3WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# == Plot trainset distribution ==\n",
        "\n",
        "extracted_labels_testset = []\n",
        "for label in Y_train:\n",
        "    for index, _ in enumerate(label):\n",
        "        if label[index]:\n",
        "            extracted_labels_testset.append(index)\n",
        "\n",
        "label_counts = Counter(extracted_labels_testset)\n",
        "print(label_counts)\n",
        "combined = list(zip(label_counts.keys(), label_counts.values()))\n",
        "combined.sort(key=lambda x: x[1], reverse=True)\n",
        "label_keys, label_values = zip(*combined)\n",
        "label_keys = list(label_keys)\n",
        "label_values = list(label_values)\n",
        "\n",
        "label_keys = list(map(lambda x: str(x), label_keys))\n",
        "\n",
        "# for index, key in enumerate(label_keys):\n",
        "#    label_keys[index] = str(index + 1) + \". \" + key[0].upper() + key[1:]\n",
        "\n",
        "plt.figure(figsize=(30, 10))\n",
        "plt.bar(label_keys, label_values, color=\"#1f77b4\")\n",
        "plt.title(\"Physionet 2021 labels\")\n",
        "plt.xlabel(\"Arrhythmia type\", labelpad=7)\n",
        "plt.ylabel(\"Occurence\")\n",
        "plt.xticks(rotation=45, ha=\"right\", fontsize=16)  # (rotation='diagional')\n",
        "bars = plt.bar(label_keys, label_values, color=\"#1f77b4\")\n",
        "# Adding the counts on top of the bars\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(\n",
        "        bar.get_x() + bar.get_width() / 2,\n",
        "        yval + 5,\n",
        "        yval,\n",
        "        ha=\"center\",\n",
        "        va=\"bottom\",\n",
        "        fontsize=16,\n",
        "    )\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "QhhGpFuwM5ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# == Plot testset distribution ==\n",
        "\n",
        "extracted_labels_testset = []\n",
        "for label in Y_test:\n",
        "    for index, _ in enumerate(label):\n",
        "        if label[index]:\n",
        "            extracted_labels_testset.append(index)\n",
        "\n",
        "label_counts = Counter(extracted_labels_testset)\n",
        "print(label_counts)\n",
        "combined = list(zip(label_counts.keys(), label_counts.values()))\n",
        "combined.sort(key=lambda x: x[1], reverse=True)\n",
        "label_keys, label_values = zip(*combined)\n",
        "label_keys = list(label_keys)\n",
        "label_values = list(label_values)\n",
        "\n",
        "label_keys = list(map(lambda x: str(x), label_keys))\n",
        "\n",
        "# for index, key in enumerate(label_keys):\n",
        "#    label_keys[index] = str(index + 1) + \". \" + key[0].upper() + key[1:]\n",
        "\n",
        "plt.figure(figsize=(30, 10))\n",
        "plt.bar(label_keys, label_values, color=\"#1f77b4\")\n",
        "plt.title(\"Physionet 2021 labels\")\n",
        "plt.xlabel(\"Arrhythmia type\", labelpad=7)\n",
        "plt.ylabel(\"Occurence\")\n",
        "plt.xticks(rotation=45, ha=\"right\", fontsize=16)  # (rotation='diagional')\n",
        "bars = plt.bar(label_keys, label_values, color=\"#1f77b4\")\n",
        "# Adding the counts on top of the bars\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(\n",
        "        bar.get_x() + bar.get_width() / 2,\n",
        "        yval + 5,\n",
        "        yval,\n",
        "        ha=\"center\",\n",
        "        va=\"bottom\",\n",
        "        fontsize=16,\n",
        "    )\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "0waxNV-CTg2J"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}